# train_configs/train_aarch64_optimized.yaml
# Configuration optimized for aarch64-sbsa platforms (GH200, DGX SPARK)
# with CUDA 13.0

pretrained_model_name_or_path: "Qwen/Qwen2.5-VL-3B-Instruct"
output_dir: "output/aarch64_optimized"
logging_dir: "logs"

# Training hyperparameters
gradient_accumulation_steps: 1
mixed_precision: "bf16"
report_to: "tensorboard"
learning_rate: 1e-4
max_train_steps: 1000
checkpointing_steps: 500
seed: 42

# LoRA configuration
rank: 16

# Data configuration
data_config:
  img_dir: "data/images"
  img_size: 1024

# CUDA 13.0 Optimizations for aarch64-sbsa
# NOTE: Flash Attention 3 is NOT STABLE on CUDA 13.0 + aarch64
cuda_features:
  enable_flash_attention_3: false  # DISABLED - unstable with GCC13 on aarch64
  enable_fp8_training: false       # Test stability before enabling
  enable_tf32_compute: true        # Fully supported
  enable_cudnn_sdp: true           # Use cuDNN SDP instead of FA3

# Memory optimizations
memory_efficient_attention: true
gradient_checkpointing: false  # Disable if sufficient memory (GH200 has 480GB unified)

# FastSafeTensors
use_fastsafetensors: true
fastsafetensors_num_threads: 8

# GH200 specific: Unified memory support
unified_memory: true  # Enable for Grace Hopper unified memory architecture

# ARM64 specific optimizations
aarch64_optimizations:
  use_cudnn_sdp: true              # Stable alternative to FA3
  neon_acceleration: true          # ARM NEON SIMD
  grace_memory_model: true         # Optimize for unified memory
