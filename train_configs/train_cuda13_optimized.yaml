# train_configs/train_cuda13_optimized.yaml
# Configuration optimized for CUDA 13.0 and Hopper GPUs

pretrained_model_name_or_path: "Qwen/Qwen2.5-VL-3B-Instruct"
output_dir: "output/cuda13_optimized"
logging_dir: "logs"

# Training hyperparameters
gradient_accumulation_steps: 1
mixed_precision: "bf16"
report_to: "tensorboard"
learning_rate: 1e-4
max_train_steps: 1000
checkpointing_steps: 500
seed: 42

# LoRA configuration
rank: 16

# Data configuration
data_config:
  img_dir: "data/images"
  img_size: 1024

# CUDA 13.0 Optimizations
cuda_13_features:
  enable_flash_attention_3: true   # Hopper+ (H100, H200)
  enable_fp8_training: false       # Optional FP8
  enable_tf32_compute: true        # Ampere+
  enable_cudnn_sdp: true           # Scaled dot-product attention

# Memory optimizations
memory_efficient_attention: true
gradient_checkpointing: false  # Disable if sufficient memory

# FastSafeTensors
use_fastsafetensors: true
fastsafetensors_num_threads: 8

# Unified memory (if on DGX Spark)
unified_memory: false
