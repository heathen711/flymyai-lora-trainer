# train_configs/train_dgx_spark.yaml
# DGX Spark (Grace Hopper) optimized training configuration
# Assumes 128GB unified CPU-GPU memory

pretrained_model_name_or_path: "Qwen/Qwen2.5-VL-3B-Instruct"
output_dir: "output/dgx_spark"
logging_dir: "logs"

# Training hyperparameters
gradient_accumulation_steps: 1
mixed_precision: "bf16"
report_to: "tensorboard"
learning_rate: 1e-4
max_train_steps: 1000
checkpointing_steps: 500
seed: 42

# Optimizer settings
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-8
max_grad_norm: 1.0

# Learning rate schedule
lr_scheduler: constant
lr_warmup_steps: 10

# LoRA configuration
rank: 16

# Checkpointing
checkpoints_total_limit: 10
resume_from_checkpoint: latest

# Data configuration
data_config:
  img_dir: "data/images"
  img_size: 1024
  train_batch_size: 1
  num_workers: 4
  caption_dropout_rate: 0.1
  random_ratio: false
  caption_type: txt

# DGX Spark Unified Memory Settings
unified_memory: true
disable_cpu_offload: true
pin_memory: false
disable_quantization: true  # Not needed with 128GB unified memory
disable_gradient_checkpointing: true  # Sufficient memory available

# Memory pool configuration
memory_fraction: 0.9
expandable_segments: true

# Disable embedding caching to disk (keep in unified memory)
precompute_text_embeddings: true
precompute_image_embeddings: true
save_cache_on_disk: false  # Keep in memory

# CUDA 13.0 Features (Grace Hopper supports these)
cuda_13_features:
  enable_flash_attention_3: true   # Hopper architecture
  enable_fp8_training: false       # Can be enabled if needed
  enable_tf32_compute: true        # Better performance
  enable_cudnn_sdp: true           # Scaled dot-product attention

# FastSafeTensors (from previous optimization)
use_fastsafetensors: true
fastsafetensors_num_threads: 8

# Removed optimizations (not needed with unified memory):
# - No 8-bit Adam
# - No quantization
# - No CPU offloading
# - No gradient checkpointing
