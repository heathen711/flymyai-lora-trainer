# train_configs/train_dgx_spark_qwen_lora.yaml
# DGX Spark optimized Qwen-Image LoRA training configuration
# Settings will be overridden by train_dgx_spark_qwen_lora.py

pretrained_model_name_or_path: "Qwen/Qwen-Image"
output_dir: "output/dgx_spark_qwen_lora"
logging_dir: "logs"

# Training hyperparameters
gradient_accumulation_steps: 1
mixed_precision: "bf16"
report_to: "tensorboard"
learning_rate: 3e-4
max_train_steps: 3000
checkpointing_steps: 250
checkpoints_total_limit: 10
seed: 42

# Optimizer settings
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-8
max_grad_norm: 1.0

# Learning rate schedule
lr_scheduler: constant
lr_warmup_steps: 10

# LoRA configuration
rank: 16

# Data configuration
data_config:
  img_dir: "data/images"  # UPDATE THIS
  img_size: 1024
  train_batch_size: 4  # Will be overridden
  num_workers: 4  # Will be overridden
  caption_dropout_rate: 0.1
  random_ratio: false
  caption_type: txt

# Embedding cache
precompute_text_embeddings: true
precompute_image_embeddings: true
save_cache_on_disk: true

# Note: The following settings are overridden by train_dgx_spark_qwen_lora.py
# unified_memory: true
# disable_cpu_offload: true
# pin_memory: false
# disable_quantization: true
# quantize: false
# adam8bit: false
# cuda_13_features.enable_flash_attention_3: false
# cuda_13_features.enable_cudnn_sdp: true
# cuda_13_features.enable_tf32_compute: true
