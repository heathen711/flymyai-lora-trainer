pretrained_model_name_or_path: Qwen/Qwen-Image
data_config:
  train_batch_size: 1
  num_workers: 4
  img_size: 1024
  caption_dropout_rate: 0.1
  img_dir: ./your_lora_dataset
  random_ratio: false # support multi crop preprocessing
  caption_type: txt
report_to: null
train_batch_size: 1
output_dir: ./output
max_train_steps: 3000
learning_rate: 1e-4
lr_scheduler: constant
lr_warmup_steps: 10
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-8
max_grad_norm: 1.0
logging_dir: logs
mixed_precision: "bf16"
checkpointing_steps: 250
checkpoints_total_limit: 10
tracker_project_name: lora_test
resume_from_checkpoint: latest
gradient_accumulation_steps: 1
rank: 16

# CUDA 13.0 Features (requires CUDA 13.0 and compatible GPU)
cuda_features:
  enable_flash_attention_3: false  # Hopper+ only
  enable_fp8_training: false       # Hopper+ only
  enable_tf32_compute: true        # Ampere+
  enable_cudnn_sdp: true           # Scaled dot-product attention

# FastSafeTensors configuration
use_fastsafetensors: true
fastsafetensors_num_threads: 8

# Unified Memory Settings (DGX Spark)
# Set unified_memory: true for Grace Hopper systems
unified_memory: false
disable_cpu_offload: false
disable_quantization: false
disable_gradient_checkpointing: false